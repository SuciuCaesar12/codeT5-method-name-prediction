{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:01.976469Z",
     "end_time": "2023-12-25T17:41:02.083253Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = \"Salesforce/codet5p-220m-bimodal\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "path_dataset = ''\n",
    "path_to_save = ''\n",
    "filename = ''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:01.992511Z",
     "end_time": "2023-12-25T17:41:02.084772Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:02.043833Z",
     "end_time": "2023-12-25T17:41:02.624576Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files=path_dataset)['train']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:02.493786Z",
     "end_time": "2023-12-25T17:41:03.446388Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\n",
    "    'code': 'encoder_input_text',\n",
    "    'name': 'target_text'\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:03.461089Z",
     "end_time": "2023-12-25T17:41:03.525305Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove the method name from code\n",
    "def remove(example):\n",
    "    example['encoder_input_text'] = \\\n",
    "        example['encoder_input_text'].replace(example['target_text'], tokenizer.sep_token, 1)\n",
    "    return example"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:03.483298Z",
     "end_time": "2023-12-25T17:41:03.525805Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dataset.map(remove)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:03.514987Z",
     "end_time": "2023-12-25T17:41:09.101334Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add the special token [TDEC] for code-to-text generation\n",
    "def add_tdec(example):\n",
    "    example['decoder_input_text'] = '[TDEC] The name of the method is: '\n",
    "    return example"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:09.103355Z",
     "end_time": "2023-12-25T17:41:09.115295Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dataset.map(add_tdec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:09.116974Z",
     "end_time": "2023-12-25T17:41:11.763762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def concat_texts(example):\n",
    "    example['stacked_text'] = [text for text in example.values()]\n",
    "    return example"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:11.764435Z",
     "end_time": "2023-12-25T17:41:11.777260Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = \\\n",
    "    dataset.map(concat_texts, remove_columns=['encoder_input_text', 'decoder_input_text', 'target_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:11.778548Z",
     "end_time": "2023-12-25T17:41:16.190655Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "max_length = 256"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:16.192191Z",
     "end_time": "2023-12-25T17:41:16.204182Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- To enhance computational efficiency, the dataset will be preprocessed into batches. Each list of ids will be padded to the maximum length of the batch.\n",
    "- It is crucial not to shuffle the dataset after this preprocessing step. The order of examples is intricately linked to their length. Maintaining this order is particularly important when selecting a number that evenly divides the batch size. This ensures that batches of the same length are created, facilitating the use of data loaders."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_batch_stacked_text(batch):\n",
    "    stacked_texts = list(chain.from_iterable(batch['stacked_text']))\n",
    "\n",
    "    tokenized_stacked_texts = \\\n",
    "        tokenizer(stacked_texts, truncation=True, padding='longest', max_length=max_length, return_tensors='pt')\n",
    "\n",
    "    tokenized_stacked_texts = \\\n",
    "        {k: v.reshape(-1, 3, v.shape[1]) for k, v in tokenized_stacked_texts.items()}\n",
    "\n",
    "    model_inputs = {\n",
    "        \"input_ids\": tokenized_stacked_texts[\"input_ids\"][:, 0, :].tolist(),\n",
    "        \"attention_mask\": tokenized_stacked_texts[\"attention_mask\"][:, 0, :].tolist(),\n",
    "        \"decoder_input_ids\": tokenized_stacked_texts[\"input_ids\"][:, 2, :].tolist(),\n",
    "        \"decoder_attention_mask\": tokenized_stacked_texts[\"attention_mask\"][:, 2, :].tolist(),\n",
    "        \"labels\": tokenized_stacked_texts[\"input_ids\"][:, 1, :]\n",
    "    }\n",
    "    model_inputs['labels'][model_inputs['labels'] == 0] = -100\n",
    "    model_inputs['labels'] = model_inputs['labels'].tolist()\n",
    "    return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:16.205182Z",
     "end_time": "2023-12-25T17:41:16.250154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dataset \\\n",
    "    .shuffle(seed=42) \\\n",
    "    .map(tokenize_batch_stacked_text, batched=True, batch_size=batch_size, drop_last_batch=True, remove_columns=['stacked_text']) \\\n",
    "    .select(range(len(dataset) - len(dataset) % batch_size))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:16.222602Z",
     "end_time": "2023-12-25T17:41:42.292285Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.to_json(os.path.join(path_to_save, filename + '.jsonl'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:42.294189Z",
     "end_time": "2023-12-25T17:41:48.049135Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_save, filename + '.json'), 'w') as f:\n",
    "    json.dump({'batch_size': batch_size, 'max_length': max_length}, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-25T17:41:48.047134Z",
     "end_time": "2023-12-25T17:41:48.063148Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
