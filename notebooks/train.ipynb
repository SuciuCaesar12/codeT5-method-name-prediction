{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T19:10:02.299420Z",
     "end_time": "2023-12-18T19:10:09.219127Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import gc\n",
    "import os\n",
    "\n",
    "checkpoint = \"Salesforce/codet5p-220m-bimodal\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_camel_case(method_name):\n",
    "    pattern = re.compile(r'(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])')\n",
    "    words = re.split(pattern, method_name)\n",
    "    return ' '.join(words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class IntellijCodeDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path_to_data, tokenizer, max_length=512):\n",
    "        self.path_to_data = path_to_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        df = pd.read_csv(path_to_data)\n",
    "        self.methods = df[\"method\"][0:10].tolist()\n",
    "        self.method_names = df[\"method_name\"][0:10].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.method_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        m = self.methods[index]\n",
    "        m_n = self.method_names[index]\n",
    "\n",
    "        # encode method\n",
    "        m = m.replace(m_n, self.tokenizer.sep_token, 1)\n",
    "        model_inputs = self.tokenizer(m, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "        # encode method name and set as target\n",
    "        labels = tokenizer(split_camel_case(m_n), return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        model_inputs[\"labels\"] = labels\n",
    "\n",
    "        # decoder inputs\n",
    "        decoder_inputs = self.tokenizer([\"[TDEC]\"], return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        model_inputs[\"decoder_input_ids\"] = decoder_inputs[\"input_ids\"]\n",
    "        model_inputs[\"decoder_attention_mask\"] = decoder_inputs[\"attention_mask\"]\n",
    "\n",
    "        return model_inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-18T19:10:09.223692Z",
     "end_time": "2023-12-18T19:10:09.253797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_to_save = \"../experiments/\"\n",
    "path_to_data = \"../data/train.csv\"\n",
    "model_save_name = \"codet5p-220m-bimodal\"\n",
    "batch_size = 1\n",
    "epochs = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 44\u001B[0m\n\u001B[0;32m     35\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\n\u001B[0;32m     36\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m     37\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     40\u001B[0m     decoder_attention_mask\u001B[38;5;241m=\u001B[39mdecoder_attention_mask\n\u001B[0;32m     41\u001B[0m )\n\u001B[0;32m     43\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m---> 44\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     48\u001B[0m pba\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(epoch) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Batch: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(i) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\jetbrains_task_env\\lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\jetbrains_task_env\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "p = pathlib.Path(path_to_save)\n",
    "p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.train()\n",
    "dataset = IntellijCodeDataset(path_to_data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pba = tqdm(dataloader)\n",
    "    i = 0\n",
    "    for batch in pba:\n",
    "        optimizer.zero_grad()\n",
    "        # move batch to device\n",
    "        input_ids = batch[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].squeeze(1).to(device)\n",
    "        labels = batch[\"labels\"].squeeze(1).to(device)\n",
    "        decoder_input_ids = batch[\"decoder_input_ids\"].squeeze(1).to(device)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].squeeze(1).to(device)\n",
    "        # inference\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "        # compute loss and backprop\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # update progress bar\n",
    "        pba.set_description(\"Epoch: \" + str(epoch) + \" Batch: \" + str(i) + f\" Loss: {loss.item():.4f}\")\n",
    "        i += 1\n",
    "\n",
    "    # save model at end of epoch\n",
    "    model.save_pretrained(os.path.join(path_to_save, model_save_name + \"-\" + str(epoch)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
